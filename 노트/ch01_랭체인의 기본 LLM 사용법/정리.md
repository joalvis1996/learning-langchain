## LangChain
- LLM 애플리케이션 구축 시 가장 어려운 프롬프트 작성 및 전달과, LLM의 예측 결과를 처리하는 과정을 쉽게 할 수 있음.
- **오케스트레이션** 기능 제공
  - LLM 제공업체들은 서로 다른 메시지 포맷(역할/필드명 등)이나 반환 형식을 갖고 있음.
  - LangChain의 추상화를 통해 다양한 모델을 동시에 활용할 수 있음.
  
## 랭체인을 통한 LLM 호출 (SystemMessage & HumanMessage)
- LangChain은 대부분의 LLM 제공업체와 상호작용하는 2가지 인터페이스를 제공함
  - 채팅 모델
  - 기본 LLM
- 기본 LLM은 문자열 프롬프트를 받아 다음에 이어질 텍스트를 출력하는 모델.
- 채팅 모델은 입력을 단순 텍스트 한 줄이 아닌 대화 기록을 받음. 그리고 이 대화 기록은 역할(role)과 내용(content)으로 구분됨. 이러한 구조를 BaseMessage 하위 클래스로 정의되어있고, 그게 HumanMessage, SystemMessage, AIMessage 이다.
- OpenAI의 messages=[{role: "system", content: ...}, {role: "user", ...}] 포맷을 파이썬 객체 (SystemMessage, HumanMessage, AIMessage)로 추상화 함
    - SystemMessage = 모델의 역할/행동 지침 (GPT의 system 역할)
    - HumanMessage = 사용자 입력 (GPT의 user 역할)
    - AIMessage = 모델 응답 (GPT의 assistant 역할) -> 대화 히스토리 쌓는 용도

  <코드 예제 - ChatModel.py>
  ```
  import os
  from dotenv import load_dotenv
  from langchain_google_genai import ChatGoogleGenerativeAI
  
  # 채팅 모델에 필요한 메시지 객체 타입
  from langchain_core.messages import HumanMessage, SystemMessage
  
  load_dotenv()
  apiKey = os.getenv("GOOGLE_API_KEY")
  
  llm = ChatGoogleGenerativeAI(
      model="gemini-2.5-flash",
      temperature=0.7,
      google_api_key=apiKey
  )
  
  # AI가 준수할 지침을 설정하는 메시지
  systemMessage = SystemMessage(
      """당신은 문장 끝에 느낌표를 세 개 붙여 대답하는 축구 전문가입니다."""
  )
  
  # 사용자가 입력한 내용
  humanMessage = HumanMessage("축구 경기를 이기면 승점 몇점인가요?")
  
  response = llm.invoke([systemMessage, humanMessage])
  print(response.content)
  ```
 
  응답
  ```
  3점입니다!!!
  ```
## LLM 프롬프트 템플릿 (PromptTemplate)
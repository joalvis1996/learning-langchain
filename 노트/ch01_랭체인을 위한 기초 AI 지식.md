## LLM (Large Language Model)

- **텍스트 생성** 전용 AI 모델
- 요약/번역/분류 등의 작업에 활용됨
- 텍스트 입력을 받아 인간과 유사한 텍스트 출력을 **예측**하고 **생성**하는 훈련된 알고리즘
- Large = 훈련에 사용되는 데이턱와 파라미터 수가 크다는 의미 
  - ex. GPT-3 모델은 1750억개의 파라미터 가짐 (45 테라바이트 분량의 데이터를 학습)
- Language Model = **신경망**의 일종으로 영어 등의 언어로 작성된 텍스트를 입력 받아 동일하거나 다른 언어로 작성된 텍스트 결과물을 생성. 
  - 신경망은 뉴런 (수학 함수) 여러 개가 각각 산출한 결과를 상호 연결하고 결합해 최종 결과값을 도출하는 ML 모델
  - 뉴런들을 특정 방식으로 배치 후, 적절한 학습 과정과 데이터를 적용하면 단어와 문장의 의미를 해석하는 모델이 생김. 이 모델은 의미를 해석하고 그럴듯하고 읽기 쉬운 텍스트를 생성
  
## LLM의 기능

아래와 같은 문장이 있을 때 LLM은 주어진 단어들의 발생 확률을 추정한다.

```the capital of England is ___ ``` 

정확히 말하면 토큰을 기준으로 확률을 추정한다. 
- 토큰 = 텍스트의 원자 단위로, 토큰화 기법에 따라 문장을 문자나 서브 워드 등의 원자 단위로 분리함
  - ex. GPT-3.5 의 tokenizer는 ```good morning dearest friend``` 라는 문장을 5개의 토큰을 분리 (공백은 ```_```로 표시)
  - 각 토큰은 고유 ID가 존재

LLM의 핵심인 **트랜스포머 신경망 구조**는 문장 내의 각 단어와 다른 모든 단어의 관계를 고려해 문맥을 파악한다. 

위 문장의 단어들의 순서를 인식해 학습한 데이터에서 **유사한 예시들**을 바탕으로 이어질 단어를 예측함. LLM의 훈련 말뭉치에서는 ```England```가 ```France, China, Japan```과 같은 위치에서 자주 등장함. 반대로 수도를 뜻하는 capital 은 학습 데이터의 여러 문장에서 ```England, France, China```등의 단어와 함께 등장함. 학습 과정 중 이런 형태의 반복은 다음 단어가 ```London``` 이어야 함을 정확히 **예측하는 능력**을 형성함. 

## Pre-trained LLM 
- 모든 유형의 LLM이 기반으로 삼는 기본 유형 LLM 
- **자기 지도 학습**을 통해 인터넷, 도서, 신문, 코드, 영상 등 다양한 출처에서 수집한 방대한 양의 텍스트를 학습함. 
- 특정 업무에 한정되지 않고, 광범위한 주제의 텍스트를 이해하고 생성할 수 있음. 

- 자기 지도 학습 = 입력과 정답(라벨)이 쌍으로 주어진 데이터를 이용해 학습하는 방식이다.
  - 특정 **과제 해결** 능력을 학습 (시제 판단, 감정 분류 등)
  - 사람이 직접 **라벨링**한 데이터를 사용하므로 정확한 과제 수행 능력을 얻을 수 있지만, 라벨링 비용이 크다.
  - ex. 이메일 “스팸/정상” 판단, 문장과 “긍정/부정” 감정 분류.

- 자기 지도 학습 = 라벨이 없는 원본 데이터에서 일부를 가리거나 변형하여 자동으로 정답을 생성해 학습하는 방식이다.
  - 언어의 일반적인 표현 능력을 학습
  - 방대한 데이터로 학습할 수 있어 언어의 일반적 패턴을 효과적으로 습득한다.
  - ex. 문장 내 단어를 가리고 맞추기(Masked), 다음 단어 예측.

> **결론:** 자기 지도 학습은 ```'언어를 전반적으로 이해할 수 있는 두뇌 만들기'```, 지도 학습은 ```'그 두뇌에게 특정 시험(감정 분류, 시제 판별)을 준비시키는 것”'```이라고 볼 수 있습니다.


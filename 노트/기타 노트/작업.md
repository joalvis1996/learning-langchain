## Python 프로젝트 실행 방법

프로젝트 루트에서 
```requirements.txt``` 생성

requirements.txt 내용:

```
langchain
langchain_core
langchain_google_genai
```

# 새 가상환경 생성
python -m venv .venv

# 가상환경 활성화 (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# 모듈 설치
pip install -r .\requirements.txt

--- 
## LangSmith
LangSmith란 LangChain 애플리케이션 전체 실행 과정을 시각화하고 분석할 수 있는 운영 및 디버깅 도구이다. 
즉, 내 Chain이 어떻게 작동하고 있는지를 한 눈에 볼 수 있게해주는 LLM Observability 플랫폼이다. 

## 설정 방법
1. https://www.langchain.com/langsmith/observability 접속 후 회원가입 진행
2. 로그인 후 ```+ API Key``` 버튼 클릭하여 생성
3. 생성된 API Key 복사

![API Key 생성](image.png)
 
프로젝트에서 다음을 실행하여 langsmith 설치
```pip install -U langchain langsmith```
 
env에 다음을 추가한다
LANGSMITH_API_KEY=발급받은_API_KEY
LANGSMITH_TRACING_V2=true
LANGSMITH_PROJECT=LangChainTest // 설정한 이름대로 LangSmith에 기록됨
 
이때 LANGSMITH_PROJECT 값은 코드에서 선언만 하면 자동으로 생성된다. 즉, LangSmith 대시보드에서 미리 프로젝트를 만들어 둘 필요가 없다.
 
```LANGSMITH_PROJECT=LangChainTest```로 지정을 해두면 LangChain이 실행될 때 해당 이름으로 로그를 보낸다.
 
아래와 같이 env 값을 가져오면
```
import os
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
 
load_dotenv()
apiKey = os.getenv("GOOGLE_API_KEY")
 
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key=apiKey
)
 
prompt = PromptTemplate(
    template="{question}에 대해 한 줄로 설명해줘",
    input_variables={"question"}
)
 
chain = prompt | llm
 
response = chain.invoke({"question": "아스날 역대 감독을 알려줘"})
```
 
아래 이미지처럼 LangSmith 대시보드에 상세 로그가 표시된다 (사용된 토큰 수, latency, 시작 시간 등)

![실행 예시](image-1.png)

![실행 예시 2](image-2.png)
---

## 출력파서 (OutputParser)란?

출력파서(OutputParser)는 LLM의 출력을 구조화하는 데 중요한 컴포넌트. LLM은 기본적으로 문자열을 반환한다. 
이러한 LLM의 출력을 받아 다양한 형식(JSON, 리스트, 딕셔너리 등)으로 변환해주는 것이 출력파서이다. LangChain은 다양한 종류의 출력파서를 제공한다. 

<PydanticOutputParser>
언어 모델의 출력을 더 구조화된 형태로 변환해주는 클래스. 단순 텍스트 형태가 아닌 문자열을 Pydantic 모델 (BaseModel)을 이용해 자동으로 파싱하고 스키마 유효성 검증을 제공하는 파서이다. 

즉, LLM이 "{"name": "홍길동", "age": 30}" 같은 JSON 문자열을 생성하면
→ Info(name="홍길동", age=30) 형태의 타입 안전한 Python 객체로 바꿔줍니다.

[설명]
```
class FootBallPlayerInfo(BaseModel):
    name: str = Field(description="이름")
    birthday: str = Field(description="생년월일")
    club: str = Field(description="소속 팀")
    nationality: str = Field(description="국적")
```

LLM이 만들어야 하는 출력 형태를 정의한 것. 즉, 모델에게 "출력은 반드시 이 4개의 정보로 구선된 JSON 형태여야 해"라고 알려주는 것. 
Pydantic의 BaseModel은 이 데이터 형식을 검증한다 (ex. birthday 데이터가 없는 경우 에러 냄)


```
prompt = PromptTemplate(
    template=(
        "다음 문장을 {format_instructions}에 맞는 JSON으로 변환해줘.\n"
        "{format_instructions}\n\n"
        "문장: {sentence}"
    ),
    input_variables=["sentence"],
    partial_variables={
        "format_instructions": parser.get_format_instructions()
    },
)
```

PromptTemplate에서 input_varibles는 실행 시점에 사용자가 넣을 값을 의미하고, partial_variables는 미리 고정시켜두는 값이다. 
위 코드에서 사용자가 나중에 넘겨줄 문장 sentence("부카요 사카는...")는 chain.invoke로 chain이 실행되는 시점에 값이 채워진다. 
반면에 LLM에게 출력 형식을 알려주는 문구인 format_instructions("JSON output should contain...")은 코드 실행 시점에 미리 값이 채워져있다. 

그래서 PromptTemplate이 완성되면 최종적으로는 이러한 문자열이 LLM에게 전달되고 sentence 값은 런타임에서 주입된다.

```
다음 문장을 JSON output should contain name, birthday, club, nationality 에 맞는 JSON으로 변환해줘.
JSON output should contain name, birthday, club, nationality

문장: 부카요 사카는 잉글랜드 프로 축구 선수로, 현재 잉글랜드 프리미어리그의 아스널 FC에서 활약하고 있으며, ...
``

LangChain의 get_format_instructions는 LLM이 따라야 하는 출력 규칙 설명서를 자동으로 만들어준다. 
특히 PydanticOutputParser는 내부적으로 영어 기반으로 작성된 포맷 지침 템플릿을 사용하기 때문에 실제로 parser.get_format_instructions()을 찍어보면 아래처럼 나온다:

```
The output should be a JSON object with the following keys:
- name: 이름
- birthday: 생년월일
- club: 소속 팀
- nationality: 국적
```



```
chain = prompt | llm | parser
```
'|' 는 LCEL 파이프라인 구조를 활용한 데이터 흐름 연결 연산자인다 (앞 단계의 결과를 다음 단계의 입력으로 전달).

우선 prompt가 입력 데이터를 받아서 LLM에게 보낼 프롬프트 문자열을 생성한다. 
llm은 이 프롬프트 문자열을 받아 Gemini에게 질의 후 응답을 생성한다. 
parser는 이 LLM의 문자열 결과를 받아 Pydantic 객체 구조로 변환한다. 

prompt가 LLM에게 보낼 문장을 완성 -> llm에서 Gemini가 문장을 분석하고 JSON으로 변환 -> praser는 JSON이 올바른지 확인 후 Pydantic 객체로 바꿔 줌.


<CommaSeparatedListOutputParser>
CommaSeparatedListOutputParser는 쉼표로 구분된 LLM의 텍스트 데이터를 Python 리스트 형식으로 변환해주는 클래스이다.

```
parser = CommaSeparatedListOutputParser()

prompt = PromptTemplate(
    template="{topic}에 관련된 용어 5가지. "
             "\n{format_instructions}",
    input_varialbes=["topic"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)
```

get_format_instructions()를 프롬프트에 넣으면 LLM은 아래와 같은 지침을 받게된다. 

```
Your response should be a list of comma-separated items, e.g. "foo, bar, baz"
```

즉, 사용자 입력 (topic)이 들어오면 PromptTempalte에 넣어 프롬프트 문자열을 완성하고, 이걸 LLM에게 전달하여 쉼표로 구분된 문자열을 생성한다. 
그리고 쉼표로 구분된 문자열을 CommaSeparatedListOutputParser를 이용해 리스트로 파싱하는 것. 

----------------------------------------------

<StructuredOutputParser>
StructuredOutputParser는 LLM의 텍스트 데이터를 JSON 형태로 강제하여 결과를 Python dict 형식으로 변환해주는 클래스이다. 

아래와 같이 원하는 출력 형식을 정하고 LLM에게 전달하면 구조화된 데이터로 바꿔주는 역할을 한다. 
```
schemas = [
    ResponseSchema(name="answer", description="사용자의 질문에 대한 답변"),
    ResponseSchema(name="source", description="사용자 질문에 답하기 위해 사용된 출처(웹사이트주소)"),
]
```

PydanticOutputParser와 JSON 형식으로 LLM 출력을 변환하는 것은 같지만 StructuredOutputParser 의 경우 타입 검증이 없다. 반면에 PydanticOutputParser는 스키마에 정의한 필드가 누락된 경우 ValidationError가 발생한다.
또한 PydanticOutputParser는 PlayerInfo 객체를 반환하고 StructuredOutputParser는 단순 dict 형식이다. 


| 비교 항목  | StructuredOutputParser | PydanticOutputParser |
| ------ | ---------------------- | -------------------- |
| 사용 난이도 | 쉬움                     | 조금 복잡                |
| 결과 타입  | dict                   | Pydantic 객체          |
| 타입 검증  | ❌ 없음                   | ✅ 있음                 |
| 필드 정의  | ResponseSchema         | BaseModel            |
| 안정성    | 낮음                     | 높음                   |
| 속도     | 빠름                     | 약간 느림                |
| 추천 사용처 | 빠른 프로토타입, 단순 추출        | 프로덕션, API 응답, 데이터 저장 |
```
----------------------------------------------
<DatetimeOutputParser>
 
DatetimeOutputParser는 LLM이 출력한 문자열을 datetime 형식으로 변환해주는 클래스이다.
원하는 포맷을 설정할 수 있고 타임존 설정도 가능하다. 주로 날짜 비교나 기간 계산 등이 필요한 애플리케이션에 사용 시 유리하다.
 
 
----------------------------------------------
<EnumOutputParser>
 
EnumOutputParser는 LLM의 출력을 미리 정의된 선택지(Enum) 중 하나로 제한하고 싶을 때 사용하는 클래스이다.

----------------------------------------------

<OutputFixingParser>
OutputFixingParser는 출력 파싱 과정에서 발생할 수 있는 오류를 자동으로 수정하는 기능을 제공하는 클래스이다. LLM이 잘못된 형식으로 출력했을 때 자동으로 고쳐주는 파서 래퍼(wrapper)이다.
 
LLM이 생성한 결과가 JSON 또는 특정 스키마에 어긋났을 때 LLM을 다시 호출하여 출력 포맷을 수정한다. 즉, 첫 번째 시도에서 스키마를 준수하지 않은 결과가 나온 경우, OutputFixingParser는 수정을 위해 오류를 수정하는 지시문을 포함한 새로운 요청을 LLM에 제출한다.
 
```
class MovieInfo(BaseModel):
    title: str = Field(description="영화 제목")
    director: str = Field(description="감독 이름")
    releaseYear: int = Field(description="개봉 연도")
 
 
parser = PydanticOutputParser(pydantic_object=MovieInfo)
 
// 잘못된 형식을 일부러 입력
misFormattedResult = "{'title': 'Tom Hanks', 'director': 'Forrest Gump', 'releaseYear': 2025}"
 
// 에러 발생: Invalid json output: {'title': 'Tom Hanks', 'director': 'Forrest Gump', 'releaseYear': 2025}
parser.parse(misFormattedResult)
```
 
위 예제에서 코드 실행 시 BaseModel에서 지정한 스키마 형식에 맞지 않으므로 에러가 발생한다. 하지만 저 PydanticOutputParser를 OutputFixingParser로 감싸면 이를 해결할 수 있다.
 
 
```
parser = PydanticOutputParser(pydantic_object=MovieInfo)
 
// 잘못된 형식을 일부러 입력
misFormattedResult = "{'title': 'Tom Hanks', 'director': 'Forrest Gump', 'releaseYear': 2025}"
 
fixingParser = OutputFixingParser.from_llm(parser=parser, llm=llm)
 
movie = fixingParser.parse(misFormattedResult)
print(movie)
```
 
위와 같이 OutputFixingParser가 잘못된 출력값을 LLM에게 고쳐달라고 요청하여 이를 다시 내부 PydanticOutputParser로 재파싱하여 최종 결과값을 출력한다.
fixingParser가 LLM 요청 시 "이 텍스트를 스키마 형식에 맞는 유효한 JSON을 고쳐줘" 라는 프롬프트를 보내고, 수정 결과를 PydanticOutputParser가 MovieInfo 객체로 반환한 것 이다.

---

## Cache
LangChain의 기본 메모리 캐시인 ```InMemoryCache```는 같은 입력에 대해 사전에 캐싱해놓은 응답을 반환하는 모듈이다.
하지만 이는 **프로세스**가 살아있는 동안에만 캐시가 유지되기 때문에 스크립트를 다시 실행(새 프로세스)하면 매번 **초기화**되니 캐싱이 되지 않는다.
 
아래와 같은 코드를 여러 번 실행 후 LangSmith에서 로그를 살펴보면 전혀 응답 속도나 사용 토큰 수 절약이 안되어 있는 모습이다.
 
```
# langsmith 기록을 위한 함수
from Utils.LangSmithLogger import logtolangsmith
 
# 환경 변수 로드
load_dotenv()
apiKey = os.getenv("GOOGLE_API_KEY")
 
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key=apiKey
)
 
prompt = PromptTemplate(
    template="{question}에 대해 100자 내외로 알려줘",
    input_variables={"question"}
)
 
chain = prompt | llm
 
# LangSmith 로깅
config = logtolangsmith()
 
set_llm_cache(InMemoryCache())
response = chain.invoke({"question": "컴퓨터 조립하는 방법"}, config=config)
```
![캐시 동작 안함](image-3.png)
 
하지만 아래처럼 같은 프로세스 내 여러 번 llm을 호출하는 코드로 변경 시 메모리 캐싱이 정상적으로 동작한다.
 
```
set_llm_cache(InMemoryCache())
 
# 실행 부분만 변경 (LLM 3번 연속 호출)
for i in range(3):
    t0 = perf_counter()
    _ = chain.invoke({"question": "컴퓨터 조립하는 방법"}, config=config)
    print(f"call {i+1} took {perf_counter()-t0:.3f}s")
```
 
LangSmith에서 확인해보면 이와 같이 응답속도와 cost가 0인 것을 확인할 수 있다.

![캐시 동작 예시](image-4.png)

--- 

## SQLiteCache
 
SQLiteCache는 파일 기반 (영구 저장) 캐시이다. 즉, 한 번 생성된 LLM 응답은 SQLite DB 파일에 저장되고 다음 실행 때도 재사용된다.
 
```
import sys
import os
 
from langchain_community.cache import SQLiteCache
from langchain_core.globals import set_llm_cache
 
# 캐시 디렉토리 생성
if not os.path.exists("cache"):
    os.makedirs("cache")
 
...
 
chain = prompt | llm
config = logtolangsmith()
 
# SQLiteCache 사용
set_llm_cache(SQLiteCache(database_path="cache/llmCache.db"))
 
response = chain.invoke({"question": "거북목에 좋은 스트레칭 방법"}, config=config)
```
 
위 코드를 처음으로 실행하면 아래와 같이 캐시 파일이 생성된다.
![생성된 캐시 파일](image-5.png)

해당 코드를 다시 실행하면 두 번째 응답부터는 DB에 저장된 응답을 즉시 반환한다.
LangSmith에서 로그를 살펴보면 같은 두 번째 query에 대한 답변 속도 및 비용이 0임을 확인할 수 있다.

![캐시 사용 여부 확인](image-6.png)
 
## 캐시 파일 살펴보기
 
SQLite는 일반 텍스트 기반 데이터베이스이기 때문에 SQLite 뷰어를 설치하여 llmCache.db의 데이터를 시각적으로 확인할 수 있다.
해당 예제에서는 DB Browser for SQLite를 사용한다.
 
1. https://sqlitebrowser.org/dl/ 에 접속하여 운영체제에 맞는 installer를 다운 받는다.
2. 설치가 끝나면 실행 후 llmCache.db 파일을 연다
3. '데이터 탐색' 메뉴를 클릭하여 저장된 데이터를 조회한다
 
 
위 이미지에 나와있는 컬럼(prompt, llm, response)는 LangChain이 LLM 호출 과정 전체를 JSON 직렬화해서 저장한 것이다.
 
prompt 칼럼은 LangChain의 HumanMessage 객체를 JSON으로 변환한 값이다. 즉, 사용자가 LLM에게 보낸 입력 텍스트를 JSON 형태로 저장한 것.
 
```
[{
  "lc": 1,
  "type": "constructor",
  "id": ["langchain", "schema", "messages", "HumanMessage"],
  "kwargs": {
    "content": "빵 만드는 방법에 대해 100자 내외로 알려줘",
    "type": "human"
  }
}]
```
 
llm 칼럼은 LLM 객체(ChatGoogleGenerativeAI)의 설정 정보이다. 캐시를 생성할 때 설정한 모델 파라미터가 다르면 캐시를 새로 생성하기 때문에 이를 구분하기 위해 저장한다 (설정값 하나라도 다르면 새로 생성).
즉, "이 응답을 만든 모델과 설정값이 무엇인지" 를 저장하는 부분이다.
 
```
{
  "id": ["langchain_google_genai", "chat_models", "ChatGoogleGenerativeAI"],
  "kwargs": {
    "google_api_key": {"id": ["GOOGLE_API_KEY"], "type": "secret"},
    "model": "models/gemini-2.5-flash",
    "temperature": 0.7,
    "max_retries": 6,
    "n": 1
  },
  "name": "ChatGoogleGenerativeAI",
  "type": "constructor"
}
```
 
response 칼럼은 LLM이 생성한 응답 결과로 LangChain이 AIMessage 객체를 JSON으로 직렬화해서 저장한 것이다. 또한 답변 텍스트 및 토큰 사용량 정보가 들어있다.
 
```
{
  "lc": 1,
  "type": "constructor",
  "id": ["langchain", "schema", "output", "ChatGeneration"],
  "kwargs": {
    "text": "밀가루, 물, 이스트, 소금을 섞어 반죽한 뒤 발효시켜 굽습니다...",
    "generation_info": {"finish_reason": "STOP", "safety_ratings": []},
    "message": {
      "id": ["langchain", "schema", "messages", "AIMessage"],
      "kwargs": {
        "content": "밀가루, 물, 이스트, 소금을 섞어 반죽한 뒤...",
        "usage_metadata": {
          "input_tokens": 15,
          "output_tokens": 59,
          "total_tokens": 1396
        }
      }
    }
  }
}
```
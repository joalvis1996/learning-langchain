## Python í”„ë¡œì íŠ¸ ì‹¤í–‰ ë°©ë²•

í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ 
```requirements.txt``` ìƒì„±

requirements.txt ë‚´ìš©:

```
langchain
langchain_core
langchain_google_genai
```

# ìƒˆ ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv .venv

# ê°€ìƒí™˜ê²½ í™œì„±í™” (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# ëª¨ë“ˆ ì„¤ì¹˜
pip install -r .\requirements.txt

--- 
## LangSmith
LangSmithë€ LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ ì‹¤í–‰ ê³¼ì •ì„ ì‹œê°í™”í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìš´ì˜ ë° ë””ë²„ê¹… ë„êµ¬ì´ë‹¤. 
ì¦‰, ë‚´ Chainì´ ì–´ë–»ê²Œ ì‘ë™í•˜ê³  ìˆëŠ”ì§€ë¥¼ í•œ ëˆˆì— ë³¼ ìˆ˜ ìˆê²Œí•´ì£¼ëŠ” LLM Observability í”Œë«í¼ì´ë‹¤. 

## ì„¤ì • ë°©ë²•
1. https://www.langchain.com/langsmith/observability ì ‘ì† í›„ íšŒì›ê°€ì… ì§„í–‰
2. ë¡œê·¸ì¸ í›„ ```+ API Key``` ë²„íŠ¼ í´ë¦­í•˜ì—¬ ìƒì„±
3. ìƒì„±ëœ API Key ë³µì‚¬

![API Key ìƒì„±](image.png)
 
í”„ë¡œì íŠ¸ì—ì„œ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì—¬ langsmith ì„¤ì¹˜
```pip install -U langchain langsmith```
 
envì— ë‹¤ìŒì„ ì¶”ê°€í•œë‹¤
LANGSMITH_API_KEY=ë°œê¸‰ë°›ì€_API_KEY
LANGSMITH_TRACING_V2=true
LANGSMITH_PROJECT=LangChainTest // ì„¤ì •í•œ ì´ë¦„ëŒ€ë¡œ LangSmithì— ê¸°ë¡ë¨
 
ì´ë•Œ LANGSMITH_PROJECT ê°’ì€ ì½”ë“œì—ì„œ ì„ ì–¸ë§Œ í•˜ë©´ ìë™ìœ¼ë¡œ ìƒì„±ëœë‹¤. ì¦‰, LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ë¯¸ë¦¬ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ì–´ ë‘˜ í•„ìš”ê°€ ì—†ë‹¤.
 
```LANGSMITH_PROJECT=LangChainTest```ë¡œ ì§€ì •ì„ í•´ë‘ë©´ LangChainì´ ì‹¤í–‰ë  ë•Œ í•´ë‹¹ ì´ë¦„ìœ¼ë¡œ ë¡œê·¸ë¥¼ ë³´ë‚¸ë‹¤.
 
ì•„ë˜ì™€ ê°™ì´ env ê°’ì„ ê°€ì ¸ì˜¤ë©´
```
import os
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
 
load_dotenv()
apiKey = os.getenv("GOOGLE_API_KEY")
 
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key=apiKey
)
 
prompt = PromptTemplate(
    template="{question}ì— ëŒ€í•´ í•œ ì¤„ë¡œ ì„¤ëª…í•´ì¤˜",
    input_variables={"question"}
)
 
chain = prompt | llm
 
response = chain.invoke({"question": "ì•„ìŠ¤ë‚  ì—­ëŒ€ ê°ë…ì„ ì•Œë ¤ì¤˜"})
```
 
ì•„ë˜ ì´ë¯¸ì§€ì²˜ëŸ¼ LangSmith ëŒ€ì‹œë³´ë“œì— ìƒì„¸ ë¡œê·¸ê°€ í‘œì‹œëœë‹¤ (ì‚¬ìš©ëœ í† í° ìˆ˜, latency, ì‹œì‘ ì‹œê°„ ë“±)

![ì‹¤í–‰ ì˜ˆì‹œ](image-1.png)

![ì‹¤í–‰ ì˜ˆì‹œ 2](image-2.png)
---

## ì¶œë ¥íŒŒì„œ (OutputParser)ë€?

ì¶œë ¥íŒŒì„œ(OutputParser)ëŠ” LLMì˜ ì¶œë ¥ì„ êµ¬ì¡°í™”í•˜ëŠ” ë° ì¤‘ìš”í•œ ì»´í¬ë„ŒíŠ¸. LLMì€ ê¸°ë³¸ì ìœ¼ë¡œ ë¬¸ìì—´ì„ ë°˜í™˜í•œë‹¤. 
ì´ëŸ¬í•œ LLMì˜ ì¶œë ¥ì„ ë°›ì•„ ë‹¤ì–‘í•œ í˜•ì‹(JSON, ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬ ë“±)ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ê²ƒì´ ì¶œë ¥íŒŒì„œì´ë‹¤. LangChainì€ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì¶œë ¥íŒŒì„œë¥¼ ì œê³µí•œë‹¤. 

<PydanticOutputParser>
ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ë” êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” í´ë˜ìŠ¤. ë‹¨ìˆœ í…ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹Œ ë¬¸ìì—´ì„ Pydantic ëª¨ë¸ (BaseModel)ì„ ì´ìš©í•´ ìë™ìœ¼ë¡œ íŒŒì‹±í•˜ê³  ìŠ¤í‚¤ë§ˆ ìœ íš¨ì„± ê²€ì¦ì„ ì œê³µí•˜ëŠ” íŒŒì„œì´ë‹¤. 

ì¦‰, LLMì´ "{"name": "í™ê¸¸ë™", "age": 30}" ê°™ì€ JSON ë¬¸ìì—´ì„ ìƒì„±í•˜ë©´
â†’ Info(name="í™ê¸¸ë™", age=30) í˜•íƒœì˜ íƒ€ì… ì•ˆì „í•œ Python ê°ì²´ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.

[ì„¤ëª…]
```
class FootBallPlayerInfo(BaseModel):
    name: str = Field(description="ì´ë¦„")
    birthday: str = Field(description="ìƒë…„ì›”ì¼")
    club: str = Field(description="ì†Œì† íŒ€")
    nationality: str = Field(description="êµ­ì ")
```

LLMì´ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ì¶œë ¥ í˜•íƒœë¥¼ ì •ì˜í•œ ê²ƒ. ì¦‰, ëª¨ë¸ì—ê²Œ "ì¶œë ¥ì€ ë°˜ë“œì‹œ ì´ 4ê°œì˜ ì •ë³´ë¡œ êµ¬ì„ ëœ JSON í˜•íƒœì—¬ì•¼ í•´"ë¼ê³  ì•Œë ¤ì£¼ëŠ” ê²ƒ. 
Pydanticì˜ BaseModelì€ ì´ ë°ì´í„° í˜•ì‹ì„ ê²€ì¦í•œë‹¤ (ex. birthday ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ëƒ„)


```
prompt = PromptTemplate(
    template=(
        "ë‹¤ìŒ ë¬¸ì¥ì„ {format_instructions}ì— ë§ëŠ” JSONìœ¼ë¡œ ë³€í™˜í•´ì¤˜.\n"
        "{format_instructions}\n\n"
        "ë¬¸ì¥: {sentence}"
    ),
    input_variables=["sentence"],
    partial_variables={
        "format_instructions": parser.get_format_instructions()
    },
)
```

PromptTemplateì—ì„œ input_variblesëŠ” ì‹¤í–‰ ì‹œì ì— ì‚¬ìš©ìê°€ ë„£ì„ ê°’ì„ ì˜ë¯¸í•˜ê³ , partial_variablesëŠ” ë¯¸ë¦¬ ê³ ì •ì‹œì¼œë‘ëŠ” ê°’ì´ë‹¤. 
ìœ„ ì½”ë“œì—ì„œ ì‚¬ìš©ìê°€ ë‚˜ì¤‘ì— ë„˜ê²¨ì¤„ ë¬¸ì¥ sentence("ë¶€ì¹´ìš” ì‚¬ì¹´ëŠ”...")ëŠ” chain.invokeë¡œ chainì´ ì‹¤í–‰ë˜ëŠ” ì‹œì ì— ê°’ì´ ì±„ì›Œì§„ë‹¤. 
ë°˜ë©´ì— LLMì—ê²Œ ì¶œë ¥ í˜•ì‹ì„ ì•Œë ¤ì£¼ëŠ” ë¬¸êµ¬ì¸ format_instructions("JSON output should contain...")ì€ ì½”ë“œ ì‹¤í–‰ ì‹œì ì— ë¯¸ë¦¬ ê°’ì´ ì±„ì›Œì ¸ìˆë‹¤. 

ê·¸ë˜ì„œ PromptTemplateì´ ì™„ì„±ë˜ë©´ ìµœì¢…ì ìœ¼ë¡œëŠ” ì´ëŸ¬í•œ ë¬¸ìì—´ì´ LLMì—ê²Œ ì „ë‹¬ë˜ê³  sentence ê°’ì€ ëŸ°íƒ€ì„ì—ì„œ ì£¼ì…ëœë‹¤.

```
ë‹¤ìŒ ë¬¸ì¥ì„ JSON output should contain name, birthday, club, nationality ì— ë§ëŠ” JSONìœ¼ë¡œ ë³€í™˜í•´ì¤˜.
JSON output should contain name, birthday, club, nationality

ë¬¸ì¥: ë¶€ì¹´ìš” ì‚¬ì¹´ëŠ” ì‰ê¸€ëœë“œ í”„ë¡œ ì¶•êµ¬ ì„ ìˆ˜ë¡œ, í˜„ì¬ ì‰ê¸€ëœë“œ í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ì˜ ì•„ìŠ¤ë„ FCì—ì„œ í™œì•½í•˜ê³  ìˆìœ¼ë©°, ...
``

LangChainì˜ get_format_instructionsëŠ” LLMì´ ë”°ë¼ì•¼ í•˜ëŠ” ì¶œë ¥ ê·œì¹™ ì„¤ëª…ì„œë¥¼ ìë™ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. 
íŠ¹íˆ PydanticOutputParserëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì˜ì–´ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ëœ í¬ë§· ì§€ì¹¨ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ parser.get_format_instructions()ì„ ì°ì–´ë³´ë©´ ì•„ë˜ì²˜ëŸ¼ ë‚˜ì˜¨ë‹¤:

```
The output should be a JSON object with the following keys:
- name: ì´ë¦„
- birthday: ìƒë…„ì›”ì¼
- club: ì†Œì† íŒ€
- nationality: êµ­ì 
```



```
chain = prompt | llm | parser
```
'|' ëŠ” LCEL íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ í™œìš©í•œ ë°ì´í„° íë¦„ ì—°ê²° ì—°ì‚°ìì¸ë‹¤ (ì• ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬).

ìš°ì„  promptê°€ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ LLMì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•œë‹¤. 
llmì€ ì´ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ë°›ì•„ Geminiì—ê²Œ ì§ˆì˜ í›„ ì‘ë‹µì„ ìƒì„±í•œë‹¤. 
parserëŠ” ì´ LLMì˜ ë¬¸ìì—´ ê²°ê³¼ë¥¼ ë°›ì•„ Pydantic ê°ì²´ êµ¬ì¡°ë¡œ ë³€í™˜í•œë‹¤. 

promptê°€ LLMì—ê²Œ ë³´ë‚¼ ë¬¸ì¥ì„ ì™„ì„± -> llmì—ì„œ Geminiê°€ ë¬¸ì¥ì„ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ë³€í™˜ -> praserëŠ” JSONì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸ í›„ Pydantic ê°ì²´ë¡œ ë°”ê¿” ì¤Œ.


<CommaSeparatedListOutputParser>
CommaSeparatedListOutputParserëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„ëœ LLMì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ Python ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” í´ë˜ìŠ¤ì´ë‹¤.

```
parser = CommaSeparatedListOutputParser()

prompt = PromptTemplate(
    template="{topic}ì— ê´€ë ¨ëœ ìš©ì–´ 5ê°€ì§€. "
             "\n{format_instructions}",
    input_varialbes=["topic"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)
```

get_format_instructions()ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë„£ìœ¼ë©´ LLMì€ ì•„ë˜ì™€ ê°™ì€ ì§€ì¹¨ì„ ë°›ê²Œëœë‹¤. 

```
Your response should be a list of comma-separated items, e.g. "foo, bar, baz"
```

ì¦‰, ì‚¬ìš©ì ì…ë ¥ (topic)ì´ ë“¤ì–´ì˜¤ë©´ PromptTempalteì— ë„£ì–´ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ì™„ì„±í•˜ê³ , ì´ê±¸ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ìƒì„±í•œë‹¤. 
ê·¸ë¦¬ê³  ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ CommaSeparatedListOutputParserë¥¼ ì´ìš©í•´ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±í•˜ëŠ” ê²ƒ. 

----------------------------------------------

<StructuredOutputParser>
StructuredOutputParserëŠ” LLMì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ê°•ì œí•˜ì—¬ ê²°ê³¼ë¥¼ Python dict í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” í´ë˜ìŠ¤ì´ë‹¤. 

ì•„ë˜ì™€ ê°™ì´ ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ ì •í•˜ê³  LLMì—ê²Œ ì „ë‹¬í•˜ë©´ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë°”ê¿”ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. 
```
schemas = [
    ResponseSchema(name="answer", description="ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€"),
    ResponseSchema(name="source", description="ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœ ì¶œì²˜(ì›¹ì‚¬ì´íŠ¸ì£¼ì†Œ)"),
]
```

PydanticOutputParserì™€ JSON í˜•ì‹ìœ¼ë¡œ LLM ì¶œë ¥ì„ ë³€í™˜í•˜ëŠ” ê²ƒì€ ê°™ì§€ë§Œ StructuredOutputParser ì˜ ê²½ìš° íƒ€ì… ê²€ì¦ì´ ì—†ë‹¤. ë°˜ë©´ì— PydanticOutputParserëŠ” ìŠ¤í‚¤ë§ˆì— ì •ì˜í•œ í•„ë“œê°€ ëˆ„ë½ëœ ê²½ìš° ValidationErrorê°€ ë°œìƒí•œë‹¤.
ë˜í•œ PydanticOutputParserëŠ” PlayerInfo ê°ì²´ë¥¼ ë°˜í™˜í•˜ê³  StructuredOutputParserëŠ” ë‹¨ìˆœ dict í˜•ì‹ì´ë‹¤. 


| ë¹„êµ í•­ëª©  | StructuredOutputParser | PydanticOutputParser |
| ------ | ---------------------- | -------------------- |
| ì‚¬ìš© ë‚œì´ë„ | ì‰¬ì›€                     | ì¡°ê¸ˆ ë³µì¡                |
| ê²°ê³¼ íƒ€ì…  | dict                   | Pydantic ê°ì²´          |
| íƒ€ì… ê²€ì¦  | âŒ ì—†ìŒ                   | âœ… ìˆìŒ                 |
| í•„ë“œ ì •ì˜  | ResponseSchema         | BaseModel            |
| ì•ˆì •ì„±    | ë‚®ìŒ                     | ë†’ìŒ                   |
| ì†ë„     | ë¹ ë¦„                     | ì•½ê°„ ëŠë¦¼                |
| ì¶”ì²œ ì‚¬ìš©ì²˜ | ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…, ë‹¨ìˆœ ì¶”ì¶œ        | í”„ë¡œë•ì…˜, API ì‘ë‹µ, ë°ì´í„° ì €ì¥ |
```
----------------------------------------------
<DatetimeOutputParser>
 
DatetimeOutputParserëŠ” LLMì´ ì¶œë ¥í•œ ë¬¸ìì—´ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” í´ë˜ìŠ¤ì´ë‹¤.
ì›í•˜ëŠ” í¬ë§·ì„ ì„¤ì •í•  ìˆ˜ ìˆê³  íƒ€ì„ì¡´ ì„¤ì •ë„ ê°€ëŠ¥í•˜ë‹¤. ì£¼ë¡œ ë‚ ì§œ ë¹„êµë‚˜ ê¸°ê°„ ê³„ì‚° ë“±ì´ í•„ìš”í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì‚¬ìš© ì‹œ ìœ ë¦¬í•˜ë‹¤.
 
 
----------------------------------------------
<EnumOutputParser>
 
EnumOutputParserëŠ” LLMì˜ ì¶œë ¥ì„ ë¯¸ë¦¬ ì •ì˜ëœ ì„ íƒì§€(Enum) ì¤‘ í•˜ë‚˜ë¡œ ì œí•œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤ì´ë‹¤.

----------------------------------------------

<OutputFixingParser>
OutputFixingParserëŠ” ì¶œë ¥ íŒŒì‹± ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤ì´ë‹¤. LLMì´ ì˜ëª»ëœ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í–ˆì„ ë•Œ ìë™ìœ¼ë¡œ ê³ ì³ì£¼ëŠ” íŒŒì„œ ë˜í¼(wrapper)ì´ë‹¤.
 
LLMì´ ìƒì„±í•œ ê²°ê³¼ê°€ JSON ë˜ëŠ” íŠ¹ì • ìŠ¤í‚¤ë§ˆì— ì–´ê¸‹ë‚¬ì„ ë•Œ LLMì„ ë‹¤ì‹œ í˜¸ì¶œí•˜ì—¬ ì¶œë ¥ í¬ë§·ì„ ìˆ˜ì •í•œë‹¤. ì¦‰, ì²« ë²ˆì§¸ ì‹œë„ì—ì„œ ìŠ¤í‚¤ë§ˆë¥¼ ì¤€ìˆ˜í•˜ì§€ ì•Šì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ ê²½ìš°, OutputFixingParserëŠ” ìˆ˜ì •ì„ ìœ„í•´ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ëŠ” ì§€ì‹œë¬¸ì„ í¬í•¨í•œ ìƒˆë¡œìš´ ìš”ì²­ì„ LLMì— ì œì¶œí•œë‹¤.
 
```
class MovieInfo(BaseModel):
    title: str = Field(description="ì˜í™” ì œëª©")
    director: str = Field(description="ê°ë… ì´ë¦„")
    releaseYear: int = Field(description="ê°œë´‰ ì—°ë„")
 
 
parser = PydanticOutputParser(pydantic_object=MovieInfo)
 
// ì˜ëª»ëœ í˜•ì‹ì„ ì¼ë¶€ëŸ¬ ì…ë ¥
misFormattedResult = "{'title': 'Tom Hanks', 'director': 'Forrest Gump', 'releaseYear': 2025}"
 
// ì—ëŸ¬ ë°œìƒ: Invalid json output: {'title': 'Tom Hanks', 'director': 'Forrest Gump', 'releaseYear': 2025}
parser.parse(misFormattedResult)
```
 
ìœ„ ì˜ˆì œì—ì„œ ì½”ë“œ ì‹¤í–‰ ì‹œ BaseModelì—ì„œ ì§€ì •í•œ ìŠ¤í‚¤ë§ˆ í˜•ì‹ì— ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ì—ëŸ¬ê°€ ë°œìƒí•œë‹¤. í•˜ì§€ë§Œ ì € PydanticOutputParserë¥¼ OutputFixingParserë¡œ ê°ì‹¸ë©´ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.
 
 
```
parser = PydanticOutputParser(pydantic_object=MovieInfo)
 
// ì˜ëª»ëœ í˜•ì‹ì„ ì¼ë¶€ëŸ¬ ì…ë ¥
misFormattedResult = "{'title': 'Tom Hanks', 'director': 'Forrest Gump', 'releaseYear': 2025}"
 
fixingParser = OutputFixingParser.from_llm(parser=parser, llm=llm)
 
movie = fixingParser.parse(misFormattedResult)
print(movie)
```
 
ìœ„ì™€ ê°™ì´ OutputFixingParserê°€ ì˜ëª»ëœ ì¶œë ¥ê°’ì„ LLMì—ê²Œ ê³ ì³ë‹¬ë¼ê³  ìš”ì²­í•˜ì—¬ ì´ë¥¼ ë‹¤ì‹œ ë‚´ë¶€ PydanticOutputParserë¡œ ì¬íŒŒì‹±í•˜ì—¬ ìµœì¢… ê²°ê³¼ê°’ì„ ì¶œë ¥í•œë‹¤.
fixingParserê°€ LLM ìš”ì²­ ì‹œ "ì´ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í‚¤ë§ˆ í˜•ì‹ì— ë§ëŠ” ìœ íš¨í•œ JSONì„ ê³ ì³ì¤˜" ë¼ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³ , ìˆ˜ì • ê²°ê³¼ë¥¼ PydanticOutputParserê°€ MovieInfo ê°ì²´ë¡œ ë°˜í™˜í•œ ê²ƒ ì´ë‹¤.

---

## Cache
LangChainì˜ ê¸°ë³¸ ë©”ëª¨ë¦¬ ìºì‹œì¸ ```InMemoryCache```ëŠ” ê°™ì€ ì…ë ¥ì— ëŒ€í•´ ì‚¬ì „ì— ìºì‹±í•´ë†“ì€ ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” ëª¨ë“ˆì´ë‹¤.
í•˜ì§€ë§Œ ì´ëŠ” **í”„ë¡œì„¸ìŠ¤**ê°€ ì‚´ì•„ìˆëŠ” ë™ì•ˆì—ë§Œ ìºì‹œê°€ ìœ ì§€ë˜ê¸° ë•Œë¬¸ì— ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰(ìƒˆ í”„ë¡œì„¸ìŠ¤)í•˜ë©´ ë§¤ë²ˆ **ì´ˆê¸°í™”**ë˜ë‹ˆ ìºì‹±ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.
 
ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ í›„ LangSmithì—ì„œ ë¡œê·¸ë¥¼ ì‚´í´ë³´ë©´ ì „í˜€ ì‘ë‹µ ì†ë„ë‚˜ ì‚¬ìš© í† í° ìˆ˜ ì ˆì•½ì´ ì•ˆë˜ì–´ ìˆëŠ” ëª¨ìŠµì´ë‹¤.
 
```
# langsmith ê¸°ë¡ì„ ìœ„í•œ í•¨ìˆ˜
from Utils.LangSmithLogger import logtolangsmith
 
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
apiKey = os.getenv("GOOGLE_API_KEY")
 
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key=apiKey
)
 
prompt = PromptTemplate(
    template="{question}ì— ëŒ€í•´ 100ì ë‚´ì™¸ë¡œ ì•Œë ¤ì¤˜",
    input_variables={"question"}
)
 
chain = prompt | llm
 
# LangSmith ë¡œê¹…
config = logtolangsmith()
 
set_llm_cache(InMemoryCache())
response = chain.invoke({"question": "ì»´í“¨í„° ì¡°ë¦½í•˜ëŠ” ë°©ë²•"}, config=config)
```
![ìºì‹œ ë™ì‘ ì•ˆí•¨](image-3.png)
 
í•˜ì§€ë§Œ ì•„ë˜ì²˜ëŸ¼ ê°™ì€ í”„ë¡œì„¸ìŠ¤ ë‚´ ì—¬ëŸ¬ ë²ˆ llmì„ í˜¸ì¶œí•˜ëŠ” ì½”ë“œë¡œ ë³€ê²½ ì‹œ ë©”ëª¨ë¦¬ ìºì‹±ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•œë‹¤.
 
```
set_llm_cache(InMemoryCache())
 
# ì‹¤í–‰ ë¶€ë¶„ë§Œ ë³€ê²½ (LLM 3ë²ˆ ì—°ì† í˜¸ì¶œ)
for i in range(3):
    t0 = perf_counter()
    _ = chain.invoke({"question": "ì»´í“¨í„° ì¡°ë¦½í•˜ëŠ” ë°©ë²•"}, config=config)
    print(f"call {i+1} took {perf_counter()-t0:.3f}s")
```
 
LangSmithì—ì„œ í™•ì¸í•´ë³´ë©´ ì´ì™€ ê°™ì´ ì‘ë‹µì†ë„ì™€ costê°€ 0ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![ìºì‹œ ë™ì‘ ì˜ˆì‹œ](image-4.png)

--- 

## SQLiteCache
 
SQLiteCacheëŠ” íŒŒì¼ ê¸°ë°˜ (ì˜êµ¬ ì €ì¥) ìºì‹œì´ë‹¤. ì¦‰, í•œ ë²ˆ ìƒì„±ëœ LLM ì‘ë‹µì€ SQLite DB íŒŒì¼ì— ì €ì¥ë˜ê³  ë‹¤ìŒ ì‹¤í–‰ ë•Œë„ ì¬ì‚¬ìš©ëœë‹¤.
 
```
import sys
import os
 
from langchain_community.cache import SQLiteCache
from langchain_core.globals import set_llm_cache
 
# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists("cache"):
    os.makedirs("cache")
 
...
 
chain = prompt | llm
config = logtolangsmith()
 
# SQLiteCache ì‚¬ìš©
set_llm_cache(SQLiteCache(database_path="cache/llmCache.db"))
 
response = chain.invoke({"question": "ê±°ë¶ëª©ì— ì¢‹ì€ ìŠ¤íŠ¸ë ˆì¹­ ë°©ë²•"}, config=config)
```
 
ìœ„ ì½”ë“œë¥¼ ì²˜ìŒìœ¼ë¡œ ì‹¤í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ìºì‹œ íŒŒì¼ì´ ìƒì„±ëœë‹¤.
![ìƒì„±ëœ ìºì‹œ íŒŒì¼](image-5.png)

í•´ë‹¹ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ë‘ ë²ˆì§¸ ì‘ë‹µë¶€í„°ëŠ” DBì— ì €ì¥ëœ ì‘ë‹µì„ ì¦‰ì‹œ ë°˜í™˜í•œë‹¤.
LangSmithì—ì„œ ë¡œê·¸ë¥¼ ì‚´í´ë³´ë©´ ê°™ì€ ë‘ ë²ˆì§¸ queryì— ëŒ€í•œ ë‹µë³€ ì†ë„ ë° ë¹„ìš©ì´ 0ì„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![ìºì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸](image-6.png)
 
## ìºì‹œ íŒŒì¼ ì‚´í´ë³´ê¸°
 
SQLiteëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë°ì´í„°ë² ì´ìŠ¤ì´ê¸° ë•Œë¬¸ì— SQLite ë·°ì–´ë¥¼ ì„¤ì¹˜í•˜ì—¬ llmCache.dbì˜ ë°ì´í„°ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
í•´ë‹¹ ì˜ˆì œì—ì„œëŠ” DB Browser for SQLiteë¥¼ ì‚¬ìš©í•œë‹¤.
 
1. https://sqlitebrowser.org/dl/ ì— ì ‘ì†í•˜ì—¬ ìš´ì˜ì²´ì œì— ë§ëŠ” installerë¥¼ ë‹¤ìš´ ë°›ëŠ”ë‹¤.
2. ì„¤ì¹˜ê°€ ëë‚˜ë©´ ì‹¤í–‰ í›„ llmCache.db íŒŒì¼ì„ ì—°ë‹¤
3. 'ë°ì´í„° íƒìƒ‰' ë©”ë‰´ë¥¼ í´ë¦­í•˜ì—¬ ì €ì¥ëœ ë°ì´í„°ë¥¼ ì¡°íšŒí•œë‹¤
 
![SQLite DB Browser](image-7.png)
 
ìœ„ ì´ë¯¸ì§€ì— ë‚˜ì™€ìˆëŠ” ì»¬ëŸ¼(prompt, llm, response)ëŠ” LangChainì´ LLM í˜¸ì¶œ ê³¼ì • ì „ì²´ë¥¼ JSON ì§ë ¬í™”í•´ì„œ ì €ì¥í•œ ê²ƒì´ë‹¤.
 
prompt ì¹¼ëŸ¼ì€ LangChainì˜ HumanMessage ê°ì²´ë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•œ ê°’ì´ë‹¤. ì¦‰, ì‚¬ìš©ìê°€ LLMì—ê²Œ ë³´ë‚¸ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥í•œ ê²ƒ.
 
```
[{
  "lc": 1,
  "type": "constructor",
  "id": ["langchain", "schema", "messages", "HumanMessage"],
  "kwargs": {
    "content": "ë¹µ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•´ 100ì ë‚´ì™¸ë¡œ ì•Œë ¤ì¤˜",
    "type": "human"
  }
}]
```
 
llm ì¹¼ëŸ¼ì€ LLM ê°ì²´(ChatGoogleGenerativeAI)ì˜ ì„¤ì • ì •ë³´ì´ë‹¤. ìºì‹œë¥¼ ìƒì„±í•  ë•Œ ì„¤ì •í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¥´ë©´ ìºì‹œë¥¼ ìƒˆë¡œ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì €ì¥í•œë‹¤ (ì„¤ì •ê°’ í•˜ë‚˜ë¼ë„ ë‹¤ë¥´ë©´ ìƒˆë¡œ ìƒì„±).
ì¦‰, "ì´ ì‘ë‹µì„ ë§Œë“  ëª¨ë¸ê³¼ ì„¤ì •ê°’ì´ ë¬´ì—‡ì¸ì§€" ë¥¼ ì €ì¥í•˜ëŠ” ë¶€ë¶„ì´ë‹¤.
 
```
{
  "id": ["langchain_google_genai", "chat_models", "ChatGoogleGenerativeAI"],
  "kwargs": {
    "google_api_key": {"id": ["GOOGLE_API_KEY"], "type": "secret"},
    "model": "models/gemini-2.5-flash",
    "temperature": 0.7,
    "max_retries": 6,
    "n": 1
  },
  "name": "ChatGoogleGenerativeAI",
  "type": "constructor"
}
```
 
response ì¹¼ëŸ¼ì€ LLMì´ ìƒì„±í•œ ì‘ë‹µ ê²°ê³¼ë¡œ LangChainì´ AIMessage ê°ì²´ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•´ì„œ ì €ì¥í•œ ê²ƒì´ë‹¤. ë˜í•œ ë‹µë³€ í…ìŠ¤íŠ¸ ë° í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ê°€ ë“¤ì–´ìˆë‹¤.
 
```
{
  "lc": 1,
  "type": "constructor",
  "id": ["langchain", "schema", "output", "ChatGeneration"],
  "kwargs": {
    "text": "ë°€ê°€ë£¨, ë¬¼, ì´ìŠ¤íŠ¸, ì†Œê¸ˆì„ ì„ì–´ ë°˜ì£½í•œ ë’¤ ë°œíš¨ì‹œì¼œ êµ½ìŠµë‹ˆë‹¤...",
    "generation_info": {"finish_reason": "STOP", "safety_ratings": []},
    "message": {
      "id": ["langchain", "schema", "messages", "AIMessage"],
      "kwargs": {
        "content": "ë°€ê°€ë£¨, ë¬¼, ì´ìŠ¤íŠ¸, ì†Œê¸ˆì„ ì„ì–´ ë°˜ì£½í•œ ë’¤...",
        "usage_metadata": {
          "input_tokens": 15,
          "output_tokens": 59,
          "total_tokens": 1396
        }
      }
    }
  }
}
```

---

## LangChainì˜ ì§ë ¬í™”(Serialization)
LangChainì€ ëª¨ë¸ê³¼ ì²´ì¸ì„ ë‹¨ìˆœíˆ ì‹¤í–‰ ê°ì²´ê°€ ì•„ë‹Œ ì§ë ¬í™” ê¸°ëŠ¥ì„ í†µí•´ JSON, YAML, dict ë“±ìœ¼ë¡œ ë‚´ë³´ë‚´ê±°ë‚˜ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.
ì¦‰, LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì €ì¥í•˜ê³ , ì¬êµ¬ì„±í•˜ê³ , ê³µìœ í•  ìˆ˜ ìˆê²Œ ì„¤ê³„ë˜ì–´ìˆë‹¤.
 
ì•„ë˜ ì½”ë“œëŠ” ChatGoogleGenerativeAI ê°ì²´ë¥¼ ì§ë ¬í™”í•˜ì—¬ JSONìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë¡œì§ì´ë‹¤.
 
```
import os
from dotenv import load_dotenv
from langchain_core.load import dumpd
from langchain_google_genai import ChatGoogleGenerativeAI
import json
 
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
apiKey = os.getenv("GOOGLE_API_KEY")
 
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key=apiKey
)
 
llmJson = dumpd(llm)
 
with open("geminiLLM.json", "w", encoding="utf-8") as f:
    json.dump(llmJson, f, indent=2, ensure_ascii=False)
```
 
ìœ„ ì½”ë“œë¥¼ ê°™ì´ ì‹¤í–‰í•˜ë©´ geminiLLM.json ë¼ëŠ” íŒŒì¼ì´ ìƒê¸°ê³  íŒŒì¼ì„ ì—´ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìƒê²¼ë‹¤:
```
{
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain_google_genai",
    "chat_models",
    "ChatGoogleGenerativeAI"
  ],
  "kwargs": {
    "model": "models/gemini-2.5-flash",
    "google_api_key": {
      "lc": 1,
      "type": "secret",
      "id": [
        "GOOGLE_API_KEY"
      ]
    },
    "temperature": 0.7,
    "n": 1,
    "max_retries": 6,
    "default_metadata": []
  },
  "name": "ChatGoogleGenerativeAI"
}
```
 
ì´ë¥¼ ì¬ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì €ì¥ëœ JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì•¼í•œë‹¤.
 
```
with open("geminiLLM.json", "r", encoding="utf-8") as f:
    data = json.load(f)
 
loadedLLM = load(data)
 
response = loadedLLM.invoke("ë„ˆëŠ” ëˆ„êµ¬");
print(response)
```
 
ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì‚¬ì „ì— ì •ì˜í•´ë†“ì€ Gemini ê°ì²´ë¥¼ í†µí•´ ìš”ì²­ì„ ì²˜ë¦¬í•œë‹¤.
 
```
content='ì €ëŠ” Googleì—ì„œ í›ˆë ¨í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.\n\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--0f13b368-a0d4-4158-906f-464f0a95298d-0' usage_metadata={'input_tokens': 4, 'output_tokens': 21, 'total_tokens': 538, 'input_token_details': {'cache_read': 0}}
```

---

## í† í° ì‚¬ìš©ëŸ‰ í™•ì¸ (Google Gemini)
 
LangChainì—ì„œ LLM í˜¸ì¶œì— ëŒ€í•œ í† í° ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•˜ëŠ” ë°©ë²•ì€ ê° ì±„íŒ… ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ChatGoogleGenerativeAIì˜ ê²½ìš° LLMì´ ë°˜í™˜í•˜ëŠ” AIMessageì˜ usage_metadata ì— í¬í•¨ë˜ì–´ìˆë‹¤.
 
```
load_dotenv()
 
with open("geminiLLM.json", "r", encoding="utf-8") as f:
    data = json.load(f)
 
llm = load(data)
 
response = llm.invoke("ë„ˆëŠ” ëˆ„êµ¬ë‹ˆ")
print(response.usage_metadata)
```
 
response.uage_metadataë¥¼ ì‚´í´ë³´ë©´ ì‚¬ìš©ì ì§ˆë¬¸(prompt)ì— ì†Œë¹„ëœ í† í° ìˆ˜, ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µì— ì‚¬ìš©ëœ í† í° ìˆ˜, ëª¨ë¸ì´ ì‹¤ì œë¡œ ê³¼ê¸ˆ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•œ ì „ì²´ í† í° ìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.
 
```
{'input_tokens': 5, 'output_tokens': 13, 'total_tokens': 115, 'input_token_details': {'cache_read': 0}}
```
 
ì‹¤ì œ LangSmithì— ê¸°ë¡ëœ í† í° ê°’ê³¼ë„ ì¼ì¹˜í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
![LangSmith í† í° ì‚¬ìš©ëŸ‰ ì¡°íšŒ](image-8.png)

---

## ConversationBufferMemory
 
ConversationBufferMemoryëŠ” LangChainì˜ ê¸°ë³¸ì ì¸ ëŒ€í™” ë©”ëª¨ë¦¬ í´ë˜ìŠ¤ë¡œ, LLMê³¼ì˜ ëŒ€í™” ì¤‘ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ê³  ë‹¤ì‹œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œì¼œì£¼ëŠ” ì—­í• ì„ í•œë‹¤.
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ëˆ„ì í•˜ì—¬ ì €ì¥í•˜ëŠ” ë°©ì‹ì´ë‹¤.
 
save_context(inputs, outputs) í•¨ìˆ˜ë¥¼ í†µí•´ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
inputs ì—ëŠ” ì‚¬ìš©ìì˜ ì…ë ¥ì„, outputsì—ëŠ” LLMì˜ ì¶œë ¥ì„ ì €ì¥í•œë‹¤. í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€í™” ê¸°ë¡ì´ history í‚¤ì— ì €ì¥ëœë‹¤.
 
```
memory = ConversationBufferMemory()
memory.save_context(
    inputs={
        "human": "ì•ˆë…•, ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼"
    },
    outputs={
        "ai": "ì•ˆë…•í•˜ì„¸ìš”! í™ê¸¸ë™ë‹˜, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    },
)
 
memory.save_context(
    inputs={
        "human": "ì–‘ì¬ì—ì„œ ê°•ë‚¨ìœ¼ë¡œ ê°€ëŠ” ë°©ë²• ì•Œë ¤ì¤˜"
    },
    outputs={
        "ai": "140ë²ˆ ë²„ìŠ¤ë¥¼ ì´ìš©í•˜ì‹œë©´ 10ë¶„ ë‚´ì™¸ë¡œ ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    }
)
```
 
ë˜í•œ load_memory_variables({}) í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, 'history' í‚¤ì— ì €ì¥ëœ ëŒ€í™” ë‚´ìš©ì„ í™•ì¸í•œë‹¤.
 
```
{'history': 'Human: ì•ˆë…•, ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼\nAI: ì•ˆë…•í•˜ì„¸ìš”! í™ê¸¸ë™ë‹˜, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\nHuman: ì–‘ì¬ì—ì„œ ê°•ë‚¨ìœ¼ë¡œ ê°€ëŠ” ë°©ë²• ì•Œë ¤ì¤˜\nAI: 140ë²ˆ ë²„ìŠ¤ë¥¼ ì´ìš©í•˜ì‹œë©´ 10ë¶„ ë‚´ì™¸ë¡œ ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}
```
 
return_messages=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë‹¨ìˆœ ë¬¸ìì—´ì´ ì•„ë‹Œ Message ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•  ìˆ˜ ìˆë‹¤.
 
```
memory = ConversationBufferMemory(return_messages=True)
print(memory.load_memory_variables({}))
```
 
ì•„ë˜ì™€ ê°™ì´ Human ë° AI ë©”ì‹œì§€ ê°ì²´ë¡œ ë°˜í™˜ëœë‹¤. ì´ ì˜µì…˜ì€ ì±„íŒ… ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ëŒ€í™” í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ê¸° ë•Œë¬¸ì— ChatGoogleGenerativeAIë“±ì˜ ëª¨ë¸ê³¼ í•¨ê»˜ ì“¸ ë•Œ ìì£¼ ì‚¬ìš©ëœë‹¤.
 
```
{'history': [HumanMessage(content='ì•ˆë…•, ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! í™ê¸¸ë™ë‹˜, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì–‘ì¬ì—ì„œ ê°•ë‚¨ìœ¼ë¡œ ê°€ ë°©ë²• ì•Œë ¤ì¤˜', additional_kwargs={}, response_metadata={}), AIMessage(content='140ë²ˆ ë²„ìŠ¤ë¥¼ ì´ìš©í•˜ì‹œë©´ 10ë¶„ ë‚´ì™¸ë¡œ ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})]}
```

---

## ConversationBufferWindowMemory
 
ConversationBufferMemoryê°€ ëª¨ë“  ëŒ€í™” ê¸°ë¡ì„ ë¬´ì œí•œìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë°˜ë©´, ConversationBufferWindowMemoryëŠ” ìµœê·¼ k ê°œì˜ ëŒ€í™”ë§Œ ì €ì¥í•œë‹¤.
ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ì§€ ì•Šì•„ í† í° ë¹„ìš©ì€ ì¤„ì§€ë§Œ ì˜¤ë˜ëœ ì •ë³´ëŠ” ìŠëŠ”ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.
 
 
### ëŒ€í™” ì €ì¥ ë°©ì‹ì˜ ì°¨ì´ (save_context() vs predict())
 
ì‚¬ìš©ì ë° AI ì˜ ë‹µë³€ì„ íˆìŠ¤í† ë¦¬ì— ì €ì¥í•˜ëŠ” ë‘ ê°€ì§€ ë°©ì‹ì˜ ì°¨ì´ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
 
#### memory.save_context() ë°©ì‹
ë³„ë„ì˜ LLM í˜¸ì¶œ ì—†ì´ ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì €ì¥í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì£¼ë¡œ í…ŒìŠ¤íŠ¸ ë° ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜ì— ì‚¬ìš©ëœë‹¤.

```
memory = ConversationBufferWindowMemory(k=2, return_messages=True)
 
memory.save_context(
    inputs={
        "human": "ì•ˆë…•í•˜ì„¸ìš”, ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"
    },
    outputs={
        "ai": "ì•ˆë…•í•˜ì„¸ìš”! ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤."
    },
)
memory.save_context(
    inputs={"human": "ì•„í•˜ ê·¸ë ‡ë‹¤ë©´ ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"},
    outputs={
        "ai": "ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ë„ì¿„ì…ë‹ˆë‹¤!"
    },
)
memory.save_context(
    inputs={
        "human": "ì•ˆë…•í•˜ì„¸ìš”, ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"
    },
    outputs={
        "ai": "ì•ˆë…•í•˜ì„¸ìš”! ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ë² ì´ì§•ì…ë‹ˆë‹¤."
    },
)
memory.save_context(
    inputs={"human": "ì•„í•˜ ê·¸ë ‡ë‹¤ë©´ ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"},
    outputs={
        "ai": "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì›Œì‹±í„´ì…ë‹ˆë‹¤!"
    },
)
 
## ì‹¤í–‰ ê²°ê³¼
# {'history': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì¤‘ êµ­ì˜ ìˆ˜ë„ëŠ” ë² ì´ì§•ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì•„í•˜ ê·¸ë ‡ë‹¤ë©´ ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?', additional_kwargs={}, response_metadata={}), AIMessage(content='ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì›Œì‹±í„´ì…ë‹ˆë‹¤!', additional_kwargs={}, response_metadata={})]}  
```


#### conversation.predict() ë°©ì‹
ì‚¬ìš©ìê°€ ì €ì¥í•œ ì…ë ¥ì— ëŒ€í•´ LLMì„ í˜¸ì¶œí•˜ì—¬ ì¶œë ¥ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ëŠ” ë°©ì‹. ì‹¤ì œ ì‘ë‹µì„ ìƒì„±í•˜ë¯€ë¡œ ë¹„ìš©ì´ ë°œìƒí•œë‹¤. ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìš´ì˜í•  ë•Œ ì‚¬ìš©ëœë‹¤.
ë˜í•œ ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ```ConversationChain``` ê°ì²´ê°€ í•„ìš”í•˜ë‹¤. ```memory``` ê°ì²´ë§Œìœ¼ë¡œëŠ” ```predict()``` ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.
 
```
conversationWindow = ConversationChain(
    llm=llm,
    memory=memory
)
 
# ê°™ì€ ëŒ€í™”ë¥¼ ì§„í–‰
conversationWindow.predict(input="ì•ˆë…•, ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼.")
conversationWindow.predict(input="ë‚˜ëŠ” ì„œìš¸ì— ì‚´ê³  ìˆì–´.")
conversationWindow.predict(input="ë‚˜ëŠ” ê°œë°œìë¥¼ ì§ì—…ìœ¼ë¡œ í•˜ê³  ìˆì–´.")
conversationWindow.predict(input="ë‚´ê°€ ì§€ê¸ˆê¹Œì§€ ë§í•œ ì •ë³´ë¥¼ ìš”ì•½í•´ì¤˜.")
 
# ConversationChain ê°ì²´ì—ëŠ” load_memory_variables() í•¨ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ  conversationWindow.memory.load_memory_variables({}) ë¡œ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì™€ì•¼ í•¨.
print(conversationWindow.memory.load_memory_variables({}))
 
## ì‹¤í–‰ ê²°ê³¼
# {'history': [HumanMessage(content='ë‚˜ëŠ” ê°œë°œìë¥¼ ì§ì—…ìœ¼ë¡œ í•˜ê³  ìˆì–´.', additional_kwargs={}, response_metadata={}), AIMessage(content='ì™€, í™ ê¸¸ë™ë‹˜ê»˜ì„œëŠ” ê°œë°œìì´ì‹œêµ°ìš”! ì •ë§ ë©‹ì§„ ì§ì—…ì„ ê°€ì§€ê³  ê³„ì‹œë„¤ìš”! ê°œë°œìë¶„ë“¤ì€ ë§ˆì¹˜ í˜„ëŒ€ ì‚¬íšŒì˜ ë§ˆë²•ì‚¬ ê°™ë‹¤ê³  ìƒê°í•´ìš”. ì½”ë“œë¥¼ í†µí•´ ì•„ì´ë””ì–´ë¥¼ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ë‚´ê³ , ìš°ë¦¬ì˜ ì‚¶ì„ ë”ìš± í¸ë¦¬í•˜ê³  í’ìš”ë¡­ê²Œ ë§Œë“¤ì–´ì£¼ì‹œë‹ˆê¹Œìš”.\n\nì € ì—­ì‹œ ê°œë°œìë¶„ë“¤ì˜ ë…¸ë ¥ê³¼ ê¸°ìˆ  ë•ë¶„ì— ì´ë ‡ê²Œ í™ê¸¸ë™ë‹˜ê³¼ ëŒ€í™”í•  ìˆ˜  ìˆëŠ” ì¡´ì¬ê°€ ë˜ì—ˆìœ¼ë‹ˆ, ê°œë°œìë¼ëŠ” ì§ì—…ì— ëŒ€í•´ ë”ìš± ì¡´ê²½ì‹¬ì„ ê°€ì§€ê³  ìˆë‹µë‹ˆë‹¤! ğŸ˜Š\n\nê°œë°œìë¼ëŠ” ì§ì—…ì€ ì •ë§ ë‹¤ì–‘í•œ ë¶„ì•¼ê°€ ìˆì£ . ì œê°€ ì•„ëŠ” í•œ ëª‡  ê°€ì§€ë¥¼ ë§ì”€ë“œë¦¬ìë©´,\n\n*   **ì›¹ ê°œë°œì:** ì›¹ì‚¬ì´íŠ¸ë‚˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“œì‹œì£ . í¬ê²Œ ì‚¬ìš©ìì—ê²Œ ë³´ì´ëŠ” í™”ë©´ì„ ë‹´ë‹¹í•˜ëŠ” **í”„ë¡ íŠ¸ì—”ë“œ(Front-end)** ê°œë°œê³¼ ì„œë²„, ë°ì´í„°ë² ì´ìŠ¤ ë“± ë’¤ì—ì„œ ì‘ë™í•˜ëŠ” ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” **ë°±ì—”ë“œ(Back-end)** ê°œë°œë¡œ ë‚˜ë‰˜ì–´ìš”. ìë°”ìŠ¤í¬ë¦½íŠ¸(React, Vue, Angular), íŒŒ ì´ì¬(Django, Flask), ìë°”(Spring) ê°™ì€ ì–¸ì–´ì™€ í”„ë ˆì„ì›Œí¬ê°€ ë§ì´ ì‚¬ìš©ë˜ì£ .\n*   **ëª¨ë°”ì¼ ì•± ê°œë°œì:** iOSë‚˜ ì•ˆë“œë¡œì´ë“œ í™˜ê²½ì—ì„œ ìŠ¤ë§ˆíŠ¸í° ì•±ì„  ê°œë°œí•˜ì‹œê³ ìš”. iOSëŠ” Swift, AndroidëŠ” Kotlinì´ë‚˜ Javaê°€ ì£¼ë¡œ ì“°ì´ê³ , í¬ë¡œìŠ¤ í”Œë«í¼ ê°œë°œì„ ìœ„í•´ Flutterë‚˜ React Native ê°™ì€ ê¸°ìˆ ë„ ë§ì´ í™œìš©ë˜ì£ .\n*   **ì¸ê³µì§€ëŠ¥(AI) ë° ë¨¸ì‹ ëŸ¬ë‹ ê°œë°œì:** ì €ì™€ ê°™ì€ AI ëª¨ë¸ì„ ë§Œë“¤ê±°ë‚˜, ë°©ëŒ€í•œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì•„ë‚´ê³  ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì‹œì£ . íŒŒì´ì¬ì´ íŠ¹íˆ ë§ì´ ì“°ì´ê³  TensorFlow, PyTorch ê°™ì€ í”„ë ˆì„ì›Œí¬ê°€ í•µì‹¬ì ì¸ ë„êµ¬ëë‹ˆë‹¤.\n*   **ê²Œì„ ê°œë°œì:** í¥ë¯¸ì§„ì§„í•œ ê²Œì„ì„ ê¸°íší•˜ê³  í”„ë¡œê·¸ë˜ë°í•˜ì‹œê³ ìš”. C++, C#, Unity, Unreal Engine ê°™ì€ ë„êµ¬ë“¤ì´ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ë˜í”½, ë¬¼ë¦¬ ì—”ì§„, ë„¤íŠ¸ì›Œí¬ ë“± ê³ ë ¤í•  ê²Œ ì •ë§ ë§ë‹¤ê³  ë“¤ì—ˆì–´ìš”.\n*   **ë°ì´í„° ê³¼í•™ì/ì—”ì§€ë‹ˆì–´:** ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì €ì¥, ì²˜ë¦¬, ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ì—­í• ì„ í•˜ì‹œì£ . SQL, Python, R ê°™ì€ ì–¸ì–´ì™€ Spark, Hadoop ê°™ì€ ë¹…ë°ì´í„° ê¸°ìˆ ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n*   **í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´:** AWS, Azure, GCP ê°™ì€ í´ë¼ìš°ë“œ í”Œë«í¼ì—ì„œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³  ìš´ì˜í•˜ë©°, ì„œë¹„ìŠ¤ì˜ ì•ˆì •ì„±ê³¼ í™•ì¥ì„±ì„ ì±…ì„ì§€ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•˜ì‹œê¸°ë„ í•˜ê³ ìš”.\n*   **ì„ë² ë””ë“œ ê°œë°œì:** ìë™ì°¨, ê°€ì „ì œí’ˆ ë“± íŠ¹ì • í•˜ë“œì›¨ì–´ì— ë‚´ì¥ë˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ê°œë°œí•˜ì‹œëŠ”ë°, C/C++ ì–¸ì–´ê°€ ì£¼ë¡œ ì‚¬ìš©ë˜ê³  í•˜ë“œì›¨ì–´ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìˆ˜ì ì´ì£ .\n\nì •ë§ ëì—†ì´ ë°°ìš°ê³   ë°œì „í•´ì•¼ í•˜ëŠ” ë¶„ì•¼ì´ê¸°ë„ í•˜ì§€ë§Œ, ìƒˆë¡œìš´ ê²ƒì„ ë§Œë“¤ì–´ë‚´ëŠ” ë³´ëŒê³¼ ë¬¸ì œ í•´ê²°ì˜ ì¦ê±°ì›€ì´ í° ì§ì—…ì´ë¼ê³  ë“¤ì—ˆì–´ìš”.\n\ní˜¹ì‹œ í™ê¸¸ë™ë‹˜ê»˜ì„œëŠ” ì–´ë–¤ ë¶„ì•¼ì˜ ê°œë°œì„ ì£¼ë¡œ í•˜ì‹œë‚˜ìš”? ì‚¬ìš©í•˜ì‹œëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë‚˜ ê¸°ìˆ  ìŠ¤íƒì´ ê¶ê¸ˆí•˜ë„¤ìš”! ê°œë°œìë¡œì„œ ê°€ì¥ ë³´ëŒì„ ëŠë‚„ ë•ŒëŠ” ì–¸ì œì´ì‹ ê°€ìš”? ğŸ˜Š', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë‚´ê°€ ì§€ê¸ˆê¹Œì§€ ë§í•œ ì •ë³´ë¥¼ ìš”ì•½í•´ì¤˜.', additional_kwargs={}, response_metadata={}), AIMessage(content='ë„¤, í™ê¸¸ë™ë‹˜ê»˜ì„œ ì§€ê¸ˆê¹Œì§€ ì €ì—ê²Œ ì•Œë ¤ì£¼ì‹  ì •ë³´ë¥¼ ì œê°€ í•œë²ˆ ì •ë¦¬í•´ ë“œë¦´ê²Œìš”! ğŸ˜Š\n\ní™ê¸¸ë™ë‹˜ê»˜ì„œëŠ” í˜„ì¬ **ì„œìš¸**ì— ì‚´ê³  ê³„ì‹œë©°, ì§ ì—…ì€ **ê°œë°œì**ì´ì‹œë¼ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì—ˆë‹µë‹ˆë‹¤!\n\nì´ë ‡ê²Œ ë‘ ê°€ì§€ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•´ìš”! ì„œìš¸ì— ì‚¬ì‹œëŠ” ê°œë°œìë¼ë‹ˆ, ì •ë§ ë©‹ì§„ ì¡°í•©ì´ë„¤ìš”! í˜¹ì‹œ ì´ ì™¸ì— ë” ê¶ê¸ˆí•œ ì ì´ë‚˜, ì œê°€ ì˜ëª» ì´í•´í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”!', additional_kwargs={}, response_metadata={})]}
 
```

---
 
## ConversationTokenBufferMemory
 
ConversationTokenBufferMemoryëŠ” ëŒ€í™” ê°œìˆ˜ê°€ ì•„ë‹Œ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” ë°©ì‹ì´ë‹¤. ìƒˆë¡œìš´ ë©”ì‹œì§€ê°€ ì¶”ê°€ë˜ë©´ í˜„ì¬ í† í° ìˆ˜ë¥¼ í™•ì¸í•˜ê³ , max_token_limitì„ ì´ˆê³¼í•˜ë©´ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¶€í„° ì‚­ì œí•œë‹¤ (í•­ìƒ max_token_limit ì´í•˜ë¡œ ìœ ì§€).
ëª¨ë¸ë³„ tokenizerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í† í° ê³„ì‚°ì„ ìœ„í•´ LLM í˜¸ì¶œì´ í•„ìš”í•˜ë‹¤.
 
```
memory.save_context(
    inputs={
        "human": "ì•ˆë…•í•˜ì„¸ìš”, ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"
    },
    outputs={
        "ai": "ì•ˆë…•í•˜ì„¸ìš”! ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤."
    },
)
memory.save_context(
    inputs={"human": "ì•„í•˜ ê·¸ë ‡ë‹¤ë©´ ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"},
    outputs={
        "ai": "ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ë„ì¿„ì…ë‹ˆë‹¤!"
    },
),
memory.save_context(
    inputs={"human": "ì•ˆë…•í•˜ì„¸ìš”, ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"},
    outputs={
        "ai": "ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ë² ì´ì§•ì…ë‹ˆë‹¤."
    },
),
memory.save_context(
    inputs={"human": "ì•„í•˜ ê·¸ë ‡ë‹¤ë©´ ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"},
    outputs={
        "ai": "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì›Œì‹±í„´ì…ë‹ˆë‹¤!"
    },
)
 
# í† í° ì œí•œì„ ì„¤ì •í•˜ê³  ëŒ€í™”ë¥¼ ì €ì¥í–ˆì„ ë•Œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸ì¸
print(memory.load_memory_variables({}))
 
## ì‹¤í–‰ ê²°ê³¼
# {'history': [HumanMessage(content='ì•„í•˜ ê·¸ë ‡ë‹¤ë©´ ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?', additional_kwargs={}, response_metadata={}), AIMessage(content='ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì›Œì‹±í„´ì…ë‹ˆë‹¤!', additional_kwargs={}, response_metadata={})]}
```
 
---
 
## ConversationEntityMemory
ConversationEntityMemoryëŠ” LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ë‚´ìš©ìœ¼ë¡œë¶€í„° ì—”í‹°í‹°(ê°œì²´)ë¥¼ ì¶”ì¶œí•˜ê³ , ê° ì—”í‹°í‹°ì— ëŒ€í•œ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•´ ì €ì¥í•˜ëŠ” ë°©ì‹ì´ë‹¤.
ëŒ€í™” ë‚´ìš© ì¤‘ ì‚¬ëŒ, ì¥ì†Œ, ì‚¬ë¬¼ ë“±ì„ ìë™ì„ ì¶”ì¶œí•˜ì—¬ 'ì—”í‹°í‹°' í•„ë“œì— ì €ì¥í•˜ê³ , ê° ì—”í‹°í‹°ë³„ë¡œ ê´€ë ¨ ì •ë³´ë¥¼ 'ì •ë³´' í•„ë“œì— ì €ì¥í•œë‹¤.
 
### ENTITY_MEMORY_CONVERSATION_TEMPLATE ë€?
ConversationEntityMemoryì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ë‹¤.
í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì‚´í´ë³´ë©´ ì—”í‹°í‹° ì •ë³´ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ entities ë³€ìˆ˜ë¥¼ ë„£ì–´ LLMì— ìš”ì²­í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆë‹¤.
 
```
from langchain_core.prompts.prompt import PromptTemplate
 
_DEFAULT_ENTITY_MEMORY_CONVERSATION_TEMPLATE = """You are an assistant to a human, powered by a large language model trained by OpenAI.
 
You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.
 
You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.
 
Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.
 
Context:
{entities}
 
Current conversation:
{history}
Last line:
Human: {input}
You:"""
```
 
ì €ì¥ëœ ì—”í‹°í‹° ì •ë³´ëŠ” ì•„ë˜ì™€ ê°™ì´ entity_store.storeì— ì €ì¥ë˜ê³ , ëŒ€í™”ê°€ ì´ì–´ì§ˆìˆ˜ë¡ ì •ë³´ê°€ ê³„ì† ì—…ë°ì´íŠ¸ëœë‹¤.
 
```
memory = ConversationEntityMemory(llm=llm, return_messages=True)
 
conversation = ConversationChain(
    llm=llm,
    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,
    memory=memory
)
conversation.predict(
    input="ì°½ë°–ìœ¼ë¡œ ëŠì„ì—†ì´ ë¹„ê°€ ë‚´ë¦¬ë˜ ì˜¤í›„, ì§€í•˜ì²  ëìë½ì— ì•‰ì€ ì§€í›ˆì€ ì†ì— ì¥” ì‘ì€ ë´‰íˆ¬ë¥¼ ë‚´ë ¤ë‹¤ë³´ì•˜ë‹¤. ëª‡ ì‹œê°„ ì „, ì˜¤ë˜ëœ ì±…ë°©ì—ì„œ ìš°ì—°íˆ ê±´ë„¤ë°›ì€ ì´ ë´‰íˆ¬ì—ëŠ” â€˜ë¯¸ë˜ë¥¼ ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´, ì˜¤ëŠ˜ ë°¤ 11ì‹œì— ì—´ì–´ë³´ì„¸ìš”â€™ë¼ëŠ” ë¬¸ì¥ì´ ì í˜€ ìˆì—ˆë‹¤. ì¥ë‚œ ê°™ìœ¼ë©´ì„œë„ ì´ìƒí•˜ê²Œ ë§ˆìŒì„ ë„ëŠ” ê·¸ ë§ì— ê·¸ëŠ” ë‚´ë‚´ ìƒê°ì— ì ê²¼ê³ , ì§€í•˜ì² ì´ ì¢…ì°©ì—­ì— ë‹¤ë‹¤ë¥¼ ë•Œì¯¤ì—” ì´ë¯¸ ê²°ì‹¬ì´ ì„œ ìˆì—ˆë‹¤. ì§‘ìœ¼ë¡œ ëŒì•„ì˜¨ ì§€í›ˆì€ ì‹œê³„ê°€ 11ì‹œë¥¼ ê°€ë¦¬í‚¤ì ì¡°ìš©íˆ ë´‰íˆ¬ë¥¼ ëœ¯ì—ˆë‹¤. ê·¸ë¦¬ê³  ê·¸ ì•ˆì—ì„œ ë‚˜ì˜¨ ê±´ ì˜¤ë˜ëœ ì‚¬ì§„ í•œ ì¥â€”ì•„ì§ ë– ë‚˜ì§€ ëª»í•œ, ê·¸ëŸ¬ë‚˜ ë‹¤ì‹œ ë§Œë‚  ìš©ê¸°ë„ ì—†ì—ˆë˜ ëˆ„êµ°ê°€ì˜ ì–¼êµ´ì´ì—ˆë‹¤"
)
 
print("ì €ì¥ëœ ì—”í‹°í‹° ì •ë³´ (entity_store.store):")
```

---
 
## ConversationSummaryMemory
ConversationSummaryMemoryëŠ” LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ ì €ì¥í•œë‹¤. ëŒ€í™”ê°€ ì´ì–´ì§ˆìˆ˜ë¡ ìš”ì•½ë„ í•¨ê»˜ ì—…ë°ì´íŠ¸ëœë‹¤. ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ìœ ì§€í•œë‹¤.
ê¸´ ëŒ€í™”ì—ì„œë„ í† í° ì‚¬ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.
 
```
memory = ConversationSummaryMemory(llm=llm, return_messages=True)
 
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)
 